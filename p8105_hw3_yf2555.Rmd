---
title: "P8105 HW3"
author: "Yihan Feng"
date: "10/6/2020"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(patchwork)
knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


# Problem 1

```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and ... columns. 

Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

### Calculate number of aisles and items order. 

```{r}
instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))
```


### Plot 3 most popular items in specific aisles

```{r}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


### Table 3 most popular items in specific aisles

```{r}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```


### Table Apple and Ice cream order based on mean hour of the day

```{r}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	)
```



# Problem 2


### Load and tidy the accelerometer data. 

```{r, message = FALSE}
accel_df <- read_csv("./dataset/accel_data.csv") %>%
  janitor::clean_names() %>%
  mutate(weekday_weekend = case_when(
    day %in% c("Monday", "Tuesday", "Wednesday",  "Thursday", "Friday") ~ "Weekday",
    day %in% c("Saturday", "Sunday") ~ "Weekend"), 
    day = factor(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) %>%
  pivot_longer(
    activity_1 : activity_1440,
    names_to = "activity_time",
    names_prefix = "activity_",
    values_to = "activity_counts" 
  ) %>%
  mutate(activity_time = as.numeric(activity_time))
```

The accelerometer dataset describes the activity counts for each minute of a 24-hour day starting at midnight of a 63 year-old male with BMI 25. The dataset has `r ncol(accel_df)` variables, which are the week, day id, day, weekday or weekend, activity time, and activity counts. It includes `r nrow(accel_df)` minutes that collected by the accelerometer. 



### Create total activity variable for each day, and then a table showing these totals. 

```{r message = FALSE}
accel_day <- accel_df %>%
  group_by(week, day) %>%
  summarize(day_activity = sum(activity_counts)) %>%
  knitr::kable(digits = 1)
accel_day
```

The counts tend to be low at the beginning of each week, and then increase to the peak on Friday, and decrease at the weekend. 


### Make a single-panel plot to show 24-hour activity time courses for each day. 

```{r message = FALSE}
accel_hour <- accel_df %>%
mutate(hour_time = activity_time %/% 60) %>%
group_by(day, hour_time) %>%
summarize(hour_counts = mean(activity_counts)) %>%
  ggplot(aes(x = hour_time, y = hour_counts, color = day)) +
  geom_line() +
  scale_x_continuous(breaks = seq(0, 24, 4), 
                     labels = c("12AM", "4AM", "8AM", "12PM", "4PM", "8PM", "12AM")) + 
  scale_y_continuous(breaks = seq(0, 1000, 200)) +
  labs(
    x = "Time of the Day", 
    y = "Average Activity counts",
    title = "Activity Counts Recorded for Each Day Hourly"
  ) + 
viridis::scale_color_viridis(discrete = TRUE)
ggsave("accelerometer_hour_line.pdf", accel_hour)

accel_hour
```

According to the graph, the participant has a low and steady activity count, lower than 100, from the midnight to 4AM in the morning. In the next two hours, from 4AM to 6AM, the activity counts increases to about 400, and keeps steady until 6PM. During 6PM and 8PM, the activity counts reach to the peak of the day, and decreases gradually to 100 counts. 

# Problem 3

```{r}
library(p8105.datasets)
data("ny_noaa")
```

### Dataset Description

This dataset contains information from NOAA, National Data Center, of weather in New York. The dataset has variables of year, month, date, and weather information, which include precipitation, snow, snow wind, minimum and maximun temperature. There are a total of `r nrow(ny_noaa)` rows in the dataset, and a total of `r ncol(ny_noaa)` columns. 

### Clean the dataset. 

```{r message = FALSE}
noaa_df <- ny_noaa %>%
  janitor::clean_names() %>%
  separate(date, c("year", "month", "day"), sep = "-") %>%
    mutate(year = as.numeric(year),
           month = as.numeric(month), 
           day = as.numeric(day),
           tmax = as.numeric(tmax),
           tmax = tmax/10,
           tmin = as.numeric(tmin),
           tmin = tmin/10, 
           ) %>%
  rename("prcp_mm" = prcp, "snow_mm" = snow, "snwd_mm" = snwd, "tmax_celcius" = tmax, "tmin_celcius" = tmin)

```

```{r message = FALSE}
noaa_df %>%
  group_by(snow_mm) %>%
  count(snow_mm, name = "n_obs") %>%
  arrange(desc(n_obs))
```
For the snowfall, the most common values are 0 or NA. It might because New York is not snowing everyday. 

### Two-panel plot showing the average maximum temperature in January and in July in each station across years.

```{r message = FALSE}
noaa_jan_july <- noaa_df %>%
  filter(as.numeric(month) %in% c(1,7)) %>%
  group_by(id, year, month) %>%
  summarize(tmax_mean = mean(tmax_celcius, na.rm = TRUE)) %>%
  drop_na() %>%
ggplot(aes(x = year, y = tmax_mean, group = id, color = id)) + 
  geom_point(alpha = 0.5, show.legend = FALSE) +
  facet_grid(. ~ month) +
  scale_x_continuous(breaks = seq(1981, 2010, 3)) + 
  labs(
    x = "Year", 
    y = "Average maximum temperature (celcius)",
    title = "Average maximum temperature in January"
  ) + 
  theme(axis.text.x = element_text(angle = 90)) +
ggsave("noaa_jan_july_tmax.pdf", noaa_jan_july)
noaa_jan_july
```

```{r message = FALSE}
noaa_tmax_outlier <- noaa_df %>%
  filter(month %in% c(1,7)) %>%
  group_by(month, year, id) %>%
  summarize(tmax_mean = mean(tmax_celcius, na.rm = TRUE)) %>%
  mutate(q3 = tmax_mean > quantile(tmax_mean, 0.75, na.rm = TRUE) + 3*IQR(tmax_mean, na.rm = TRUE), 
         q1 = tmax_mean < quantile(tmax_mean, 0.25, na.rm = TRUE) - 3*IQR(tmax_mean, na.rm = TRUE)) %>%
  summarize(q3 = sum(q3, na.rm = TRUE), 
            q1 = sum(q1, na.rm = TRUE))
```


According to the plot, the mean of maximum temperature in January ranges from -10 to 10 celcius degree, and the mean of maximum temperature in July ranges from 20 to 30 celcius degree, for the majority values recorded. There are several outliers observed, which are calculated based on three times the interquatile range. The January of 1999 and 2004 are recorded for higher maximum temperature, and that of 1993, 2005, and 2008 are recorded for lower maximum temperature; the July of 1984, 1988, and 2004 are recorded for lower maximum temperature. 


### Two panel plot showing: 1. maximum and minimum temperature, 2. distribution of snowfall values greater than 0 and less than 100 by year. 

```{r message = FALSE}
noaa_min_max <- 
  ggplot(noaa_df, aes(x = tmax_celcius, y = tmin_celcius)) +
  geom_bin2d() + 
  scale_x_continuous(breaks = seq(-40, 60, 20)) +
  scale_y_continuous(breaks = seq(-40, 60, 20)) +
  labs(
    x = "maximum temperature (celcius)",
    y = "minimum temperature (celcius)",
    title = "Maximum vs. Minimum Temperature"
  )
ggsave("noaa_tmax_tmin.pdf", noaa_min_max)
```


```{r}
noaa_snow <- noaa_df %>%
  filter((snow_mm > 0) & (snow_mm < 100)) %>%
  ggplot(aes(x = as.character(year), y = snow_mm)) +
  geom_boxplot(alpha = 0.5) +
  scale_y_continuous(breaks = c(0, 50, 100)) +
  labs(
    x = "Snowfall (mm)", 
    y = "Year", 
    title = "Distribution of Snowfall values Between 0 and 100"
  ) +
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
ggsave("noaa_snow_year.pdf", noaa_snow)


noaa_min_max + noaa_snow
```
















